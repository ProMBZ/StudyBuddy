{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìö StudyBuddy\n",
        "\n",
        "**Creator:** Muhammad Bin Zohaib, Age 12, Batch 51, Lives in Karachi\n",
        "\n",
        "## Overview\n",
        "\n",
        "**StudyBuddy** is a student-focused AI assistant designed to help you with:\n",
        "- üìñ Studying\n",
        "- üìù Exam preparation\n",
        "- üîç Research on topics\n",
        "- üí™ Staying motivated\n",
        "\n",
        "It provides valuable academic assistance while keeping a friendly, encouraging tone.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Short-! Memory (üß†):** Remembers recent turns in the conversation to provide more relevant, context-aware answers.\n",
        "- **Research on Demand (üîé):**  \n",
        "  Just say \"research on this\" and StudyBuddy will use the **Tavily API** to fetch up-to-date information, along with a helpful link.\n",
        "- **Jokes (ü§£) and Quotes (üí¨):**  \n",
        "  - Say \"joke\" for a quick laugh: StudyBuddy taps into the **icanhazdadjoke API** for a random, funny joke.\n",
        "  - Say \"quote\" for inspiration: StudyBuddy offers uplifting quotes to keep you going.\n",
        "- **General Study Help (üí°):**  \n",
        "  If it‚Äôs not a research request, joke, quote, or quit command, StudyBuddy calls on **Gemini (Google Generative AI)** to answer your academic questions, explain concepts, and guide your learning.\n",
        "- **Quit Command (üö™):**  \n",
        "  Type \"quit\" to receive a final motivational message before StudyBuddy ends the session.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **User Input (üë©‚Äçüíª):**  \n",
        "   You ask a question or give a command.\n",
        "   \n",
        "2. **Keyword Check (üîë):**\n",
        "   - \"research on this\": Uses Tavily for real-time info.\n",
        "   - \"joke\": Fetches a random joke.\n",
        "   - \"quote\": Provides an encouraging quote.\n",
        "   - \"quit\": Ends the conversation with motivation.\n",
        "   - Otherwise, uses Gemini for a helpful, context-driven response.\n",
        "   \n",
        "3. **Short-Term Memory (üß†):**  \n",
        "   StudyBuddy remembers your last 5 turns to keep the conversation contextually meaningful.\n",
        "\n",
        "4. **Response (ü§ù):**  \n",
        "   StudyBuddy returns a thoughtful, study-focused answer, tailored to help you learn and grow.\n",
        "\n",
        "## Why It's Useful\n",
        "\n",
        "- **For Students (üéì):** Perfect if you‚Äôre looking for quick academic explanations, study tips, exam prep strategies, or a little motivation.\n",
        "- **Real-Time Research (üåê):** Need current info for an assignment or project? StudyBuddy has you covered.\n",
        "- **Motivation & Fun (‚ú®):** StudyBuddy‚Äôs jokes and quotes help break the monotony and keep your spirits high!\n",
        "\n",
        "With StudyBuddy by your side, every study session can be more productive, informative, and enjoyable!\n"
      ],
      "metadata": {
        "id": "QwstGYeDDGc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Required Libraries\n",
        "!pip install langchain_google_genai tavily-python langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVN2OdzS6VkA",
        "outputId": "c2aad6d8-f605-483a-ba77-caa5a0b0cca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.59-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.8.3)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.24)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tavily-python) (2.32.3)\n",
            "Collecting tiktoken>=0.5.1 (from tavily-python)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from tavily-python) (0.28.1)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.45-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.25.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.2.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->tavily-python) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->tavily-python) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->tavily-python) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->tavily-python) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (2.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->tavily-python) (1.2.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Downloading langchain_google_genai-2.0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
            "Downloading langgraph-0.2.59-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.45-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, tiktoken, tavily-python, langgraph-sdk, langgraph-checkpoint, langgraph, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain_google_genai-2.0.7 langgraph-0.2.59 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.45 tavily-python-0.5.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "i0YT0XpSM6Yg",
        "outputId": "0357be63-4236-4906-87d6-ae9902f69b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üëã Hello! I'm StudyBuddy, your dedicated study assistant.\n",
            "üìö I can help with exam preparation, study materials, research assistance (just say 'research on this'), \n",
            "   Jokes (Q&A style), quotes, study tips, facts, and subject-specific encouragement.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-64cae870b1d7>\u001b[0m in \u001b[0;36m<cell line: 260>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-64cae870b1d7>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì• You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         events = graph.stream(\n\u001b[1;32m    245\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Required libraries\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from google.colab import userdata\n",
        "from tavily import TavilyClient\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def get_quote():\n",
        "    return \"‚ÄúThe beautiful thing about learning is that nobody can take it away from you.‚Äù ‚Äì B.B. King\"\n",
        "\n",
        "def get_motivational_quote():\n",
        "    return \"Keep pushing forward in your studies, every bit of effort adds to your success!\"\n",
        "\n",
        "# Q&A style jokes list\n",
        "jokes_qa = [\n",
        "    {\"question\": \"Why was the math book sad?\", \"answer\": \"Because it had too many problems.\"},\n",
        "    {\"question\": \"What do you call cheese that isn't yours?\", \"answer\": \"Nacho cheese!\"},\n",
        "    {\"question\": \"Why don't scientists trust atoms?\", \"answer\": \"Because they make up everything!\"},\n",
        "    {\"question\": \"Why did the student eat his homework?\", \"answer\": \"Because the teacher said it was a piece of cake!\"},\n",
        "    {\"question\": \"What do you call a bear with no teeth?\", \"answer\": \"A gummy bear!\"}\n",
        "]\n",
        "\n",
        "def get_joke_qa():\n",
        "    joke = random.choice(jokes_qa)\n",
        "    return f\"Q: {joke['question']}\\nA: {joke['answer']}\"\n",
        "\n",
        "study_tips = [\n",
        "    \"Try using the Pomodoro Technique: study for 25 minutes, then rest for 5.\",\n",
        "    \"Teach the material to someone else‚Äîit helps you understand it better.\",\n",
        "    \"Break big tasks into small, manageable steps to avoid feeling overwhelmed.\",\n",
        "    \"Use flashcards for quick review sessions throughout the day.\",\n",
        "    \"Set specific, achievable study goals before you start.\"\n",
        "]\n",
        "\n",
        "def get_study_tip():\n",
        "    return random.choice(study_tips)\n",
        "\n",
        "facts = [\n",
        "    \"Did you know? The Eiffel Tower can be 15 cm taller during hot days due to thermal expansion.\",\n",
        "    \"Honey never spoils. Archaeologists have found edible honey in ancient Egyptian tombs.\",\n",
        "    \"Your brain isn't fully developed until around age 25.\",\n",
        "    \"Bananas are berries, but strawberries are not.\",\n",
        "    \"Sharks have been around longer than trees.\"\n",
        "]\n",
        "\n",
        "def get_fact():\n",
        "    return random.choice(facts)\n",
        "\n",
        "subject_prompts = {\n",
        "    \"math\": \"For math: Try to understand the concept rather than just memorizing formulas!\",\n",
        "    \"history\": \"For history: Creating a timeline of events can help you remember what happened when.\",\n",
        "    \"science\": \"For science: Relate concepts to real-life examples to make them more memorable.\",\n",
        "    \"english\": \"For English: Reading a variety of texts improves comprehension and vocabulary.\",\n",
        "    \"geography\": \"For geography: Visualize maps while studying to better recall locations.\"\n",
        "}\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')  # Ensure you've set GEMINI_API_KEY\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')  # Ensure you've set TAVILY_API_KEY\n",
        "\n",
        "# Initialize Tavily client\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "# Initialize the Gemini LLM (Google Generative AI)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY\n",
        ")\n",
        "\n",
        "tools = []\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: list\n",
        "    memory: list\n",
        "\n",
        "MAX_MEMORY_LENGTH = 5\n",
        "\n",
        "def truncate_memory(memory):\n",
        "    if len(memory) > MAX_MEMORY_LENGTH:\n",
        "        memory = memory[-MAX_MEMORY_LENGTH:]\n",
        "    return memory\n",
        "\n",
        "def summarize_memory(memory):\n",
        "    # Summarize the conversation in a \"You asked... I replied...\" format\n",
        "    user_assistant_pairs = []\n",
        "    user_msg = None\n",
        "    for turn in memory:\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            user_msg = turn[\"message\"]\n",
        "        elif turn[\"role\"] == \"assistant\" and user_msg is not None:\n",
        "            assistant_msg = turn[\"message\"]\n",
        "            user_assistant_pairs.append((user_msg, assistant_msg))\n",
        "            user_msg = None\n",
        "\n",
        "    if not user_assistant_pairs:\n",
        "        return \"No previous conversation found.\"\n",
        "\n",
        "    summary_lines = []\n",
        "    for (u, a) in user_assistant_pairs:\n",
        "        summary_lines.append(f\"You asked: '{u}'\\nI replied: '{a}'\")\n",
        "\n",
        "    return \"\\n\\n\".join(summary_lines)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    user_input = state[\"messages\"][-1][1].strip()\n",
        "    lower_input = user_input.lower()\n",
        "\n",
        "    # Append current user message to memory\n",
        "    state['memory'].append({\"role\": \"user\", \"message\": user_input})\n",
        "    state['memory'] = truncate_memory(state['memory'])\n",
        "\n",
        "    # If the user asks who made you\n",
        "    if \"who made you\" in lower_input:\n",
        "        response = \"I was created by Muhammad bin Zohaib.\"\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": response})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [response], \"memory\": state[\"memory\"]}\n",
        "\n",
        "    # Check for memory summary\n",
        "    if \"show memory\" in lower_input:\n",
        "        summary = summarize_memory(state['memory'])\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": summary})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [summary], \"memory\": state[\"memory\"]}\n",
        "\n",
        "    if \"quit\" in lower_input:\n",
        "        quote = get_motivational_quote()\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": quote})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [quote], \"memory\": state[\"memory\"], \"end\": True}\n",
        "\n",
        "    if \"joke\" in lower_input:\n",
        "        # User wants a Q&A style joke\n",
        "        joke_qa = get_joke_qa()\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": joke_qa})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [joke_qa], \"memory\": state[\"memory\"]}\n",
        "\n",
        "    if \"quote\" in lower_input:\n",
        "        quote = get_quote()\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": quote})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [quote], \"memory\": state[\"memory\"]}\n",
        "\n",
        "    if \"what did i ask before\" in lower_input:\n",
        "        previous_user_inputs = [m[\"message\"] for m in state['memory'] if m[\"role\"] == \"user\"]\n",
        "        if len(previous_user_inputs) > 1:\n",
        "            last_input = previous_user_inputs[-2]\n",
        "            response = f\"You previously asked: '{last_input}'.\"\n",
        "        else:\n",
        "            response = \"You haven't asked anything before this.\"\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": response})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [response], \"memory\": state[\"memory\"]}\n",
        "\n",
        "    if \"research on this\" in lower_input:\n",
        "        context = tavily_client.get_search_context(query=user_input)\n",
        "        answer = tavily_client.qna_search(query=user_input)\n",
        "\n",
        "        url_link = None\n",
        "        if isinstance(context, list) and len(context) > 0:\n",
        "            first_item = context[0]\n",
        "            if isinstance(first_item, str):\n",
        "                try:\n",
        "                    c_obj = json.loads(first_item)\n",
        "                    url_link = c_obj.get(\"url\", None)\n",
        "                except:\n",
        "                    url_link = None\n",
        "            elif isinstance(first_item, dict):\n",
        "                url_link = first_item.get(\"url\", None)\n",
        "\n",
        "        if url_link:\n",
        "            formatted_answer = f\"Here's some research information I found:\\n\\n{answer}\\n\\nFor more details, check this link: {url_link}\"\n",
        "        else:\n",
        "            formatted_answer = f\"Here's some research information I found:\\n\\n{answer}\"\n",
        "\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": formatted_answer})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [formatted_answer], \"memory\": state['memory']}\n",
        "\n",
        "    if \"study tip\" in lower_input:\n",
        "        tip = get_study_tip()\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": tip})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [tip], \"memory\": state['memory']}\n",
        "\n",
        "    if \"fact\" in lower_input:\n",
        "        fact = get_fact()\n",
        "        state['memory'].append({\"role\": \"assistant\", \"message\": fact})\n",
        "        state['memory'] = truncate_memory(state['memory'])\n",
        "        return {\"messages\": [fact], \"memory\": state['memory']}\n",
        "\n",
        "    # Check subject-specific prompts\n",
        "    for subject, prompt in subject_prompts.items():\n",
        "        if subject in lower_input:\n",
        "            response = prompt\n",
        "            state['memory'].append({\"role\": \"assistant\", \"message\": response})\n",
        "            state['memory'] = truncate_memory(state['memory'])\n",
        "            return {\"messages\": [response], \"memory\": state['memory']}\n",
        "\n",
        "    # Otherwise, use Gemini for a conversational or academic response\n",
        "    conversation = []\n",
        "    for turn in state['memory']:\n",
        "        conversation.append((turn[\"role\"], turn[\"message\"]))\n",
        "\n",
        "    response = llm.invoke(conversation)\n",
        "\n",
        "    if hasattr(response, 'content'):\n",
        "        response = response.content\n",
        "\n",
        "    formatted_response = response.strip() if isinstance(response, str) else str(response)\n",
        "    if not formatted_response:\n",
        "        formatted_response = \"I'm here to help with your studies. Could you clarify what you need help with today?\"\n",
        "\n",
        "    state['memory'].append({\"role\": \"assistant\", \"message\": formatted_response})\n",
        "    state['memory'] = truncate_memory(state['memory'])\n",
        "\n",
        "    return {\"messages\": [formatted_response], \"memory\": state['memory']}\n",
        "\n",
        "# Set up the graph\n",
        "graph_builder = StateGraph(State)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "def run_agent():\n",
        "    greeting = (\n",
        "        \"üëã Hello! I'm StudyBuddy, your dedicated study assistant.\\n\"\n",
        "        \"üìö I can help with exam preparation, study materials, research assistance (just say 'research on this'), \\n   Jokes (Q&A style), quotes, study tips, facts, and subject-specific encouragement.\"\n",
        "    )\n",
        "    print(greeting)\n",
        "\n",
        "    memory = []\n",
        "    while True:\n",
        "        user_input = input(\"üì• You: \")\n",
        "        events = graph.stream(\n",
        "            {\"messages\": [(\"user\", user_input)], \"memory\": memory},\n",
        "            stream_mode=\"values\"\n",
        "        )\n",
        "\n",
        "        end_chat = False\n",
        "        for event in events:\n",
        "            print(event['messages'][-1])\n",
        "            memory = event[\"memory\"]\n",
        "            if \"end\" in event:\n",
        "                end_chat = True\n",
        "\n",
        "        if end_chat:\n",
        "            print(\"Goodbye! Keep learning and growing. üìö‚úèÔ∏è\")\n",
        "            break\n",
        "\n",
        "run_agent()\n"
      ]
    }
  ]
}